{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c4b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, add\n",
    "from tensorflow.keras.layers import Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d9399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use raw string (r\"\") to avoid issues with backslashes\n",
    "images_directory = r'C:\\Users\\sunke\\Downloads\\archive (4)\\Images\\\\'\n",
    "captions_path = r'C:\\Users\\sunke\\Downloads\\archive (4)\\captions.txt'\n",
    "\n",
    "def load_captions(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        captions = f.readlines()\n",
    "        captions = [caption.lower() for caption in captions[1:]]  # skip header\n",
    "    return captions\n",
    "\n",
    "def tokenize_captions(captions):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(captions)\n",
    "    return tokenizer\n",
    "\n",
    "# Load and preview some captions\n",
    "captions = load_captions(captions_path)\n",
    "print(captions[:15:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c3b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "    \n",
    "cleaned_captions = [clean_text(caption.split(',')[1]) for caption in captions]\n",
    "cleaned_captions[:15:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_IDs = []\n",
    "for i in range(len(cleaned_captions)):\n",
    "    item = captions[i].split(',')[0]+'\\t'+'start '+cleaned_captions[i]+' end\\n'\n",
    "    captions_IDs.append(item)\n",
    "\n",
    "captions_IDs[:20:3], len(captions_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualaization(data, num_of_images):\n",
    "    captions_dictionary = {}\n",
    "    for item in data[100:100+(num_of_images)*5]:\n",
    "        image_id, caption = item.split('\\t')\n",
    "        if image_id not in captions_dictionary:\n",
    "            captions_dictionary[image_id] = []\n",
    "        captions_dictionary[image_id].append(caption)\n",
    "    else:\n",
    "        list_captions = [x for x in captions_dictionary.items()]\n",
    "\n",
    "    count = 1\n",
    "    fig = plt.figure(figsize=(10,20))\n",
    "    for filename in list(captions_dictionary.keys()):\n",
    "        captions = captions_dictionary[filename]\n",
    "        image_load = load_img(images_directory+filename, target_size=(199,199,3))\n",
    "\n",
    "        ax = fig.add_subplot(num_of_images,2,count,xticks=[],yticks=[])\n",
    "        ax.imshow(image_load)\n",
    "        count += 1\n",
    "\n",
    "        ax = fig.add_subplot(num_of_images,2,count)\n",
    "        plt.axis('off')\n",
    "        ax.plot()\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.set_ylim(0,len(captions))\n",
    "        for i, caption in enumerate(captions):\n",
    "            ax.text(0,i,caption,fontsize=20)\n",
    "        count += 1\n",
    "    plt.show()\n",
    "\n",
    "visualaization(captions_IDs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de587f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def captions_length(data):\n",
    "    plt.figure(figsize=(15, 7), dpi=300)\n",
    "    sns.set_style('darkgrid')\n",
    "    sns.histplot(x=[len(x.split(' ')) for x in data], kde=True, binwidth=1)\n",
    "    plt.title('Captions length histogram', fontsize=15, fontweight='bold')\n",
    "    plt.xticks(fontweight='bold')\n",
    "    plt.yticks(fontweight='bold')\n",
    "    plt.xlabel('Length', fontweight='bold')\n",
    "    plt.ylabel('Freaquency', fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "captions_length(cleaned_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48179c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenize_captions(cleaned_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc202b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_ids = os.listdir(images_directory)\n",
    "\n",
    "train_image_ids, val_image_ids = train_test_split(all_image_ids, test_size=0.15, random_state=42)\n",
    "val_image_ids, test_image_ids = train_test_split(val_image_ids, test_size=0.1, random_state=42)\n",
    "\n",
    "train_captions, val_captions, test_captions = [], [], []\n",
    "for caption in captions_IDs:\n",
    "    image_id, _ = caption.split('\\t')\n",
    "\n",
    "    if image_id in train_image_ids:\n",
    "        train_captions.append(caption)\n",
    "\n",
    "    elif image_id in val_image_ids:\n",
    "        val_captions.append(caption)\n",
    "\n",
    "    elif image_id in test_image_ids:\n",
    "        test_captions.append(caption)\n",
    "\n",
    "    else:\n",
    "        print('Unknown image ID !')\n",
    "\n",
    "train_captions[0], val_captions[0], test_captions[0], len(train_captions)/5, len(val_captions)/5, len(test_captions)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(299, 299))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def extract_image_features(model, image_path):\n",
    "    img = preprocess_image(image_path)\n",
    "    features = model.predict(img, verbose=0)\n",
    "    return features\n",
    "\n",
    "inception_v3_model = InceptionV3(weights = 'imagenet', input_shape=(299, 299, 3))\n",
    "inception_v3_model.layers.pop()\n",
    "inception_v3_model = Model(inputs=inception_v3_model.inputs, outputs=inception_v3_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705defa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_features, val_image_features, test_image_features = {}, {}, {}  # A Dictionary to store image features with their corresponding IDs\n",
    "pbar = tqdm(total=len(all_image_ids), position=0, leave=True, colour='green')\n",
    "\n",
    "\n",
    "for caption in all_image_ids:\n",
    "    image_id = caption.split('\\t')[0]\n",
    "    image_path = os.path.join(images_directory, image_id)\n",
    "    image_features = extract_image_features(inception_v3_model, image_path) # Extracting features\n",
    "\n",
    "    if image_id in train_image_ids:\n",
    "        train_image_features[image_id] = image_features.flatten()  # Flattening the features\n",
    "        pbar.update(1)\n",
    "\n",
    "    elif image_id in val_image_ids:\n",
    "        val_image_features[image_id] = image_features.flatten()  # Flattening the features\n",
    "        pbar.update(1)\n",
    "\n",
    "    elif image_id in test_image_ids:\n",
    "        test_image_features[image_id] = image_features.flatten()  # Flattening the features\n",
    "        pbar.update(1)\n",
    "\n",
    "    else:\n",
    "        print('Unknown image ID !')\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44723da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(captions, image_features, tokenizer, max_caption_length, batch_size):\n",
    "    num_samples = len(captions)\n",
    "    image_ids = list(image_features.keys())\n",
    "    while True:\n",
    "        np.random.shuffle(image_ids)  # Shuffle image_ids for each epoch\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, num_samples)\n",
    "            X_images, X_captions, y = [], [], []\n",
    "            for caption in captions[start_idx:end_idx]:\n",
    "                image_id, caption_text = caption.split('\\t')\n",
    "                caption_text = caption_text.rstrip('\\n')\n",
    "                seq = tokenizer.texts_to_sequences([caption_text])[0] # Tokenizing the caption\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i] # X_caption, Y\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_caption_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    X_images.append(image_features[image_id])\n",
    "                    X_captions.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "\n",
    "            yield [np.array(X_images), np.array(X_captions)], np.array(y)\n",
    "\n",
    "\n",
    "max_caption_length = max(len(caption.split()) for caption in cleaned_captions) + 1\n",
    "\n",
    "cnn_output_dim = inception_v3_model.output_shape[1] # 2048\n",
    "\n",
    "batch_size_train = 270\n",
    "batch_size_val = 150\n",
    "\n",
    "train_data_generator = data_generator(train_captions, train_image_features, tokenizer, max_caption_length, batch_size_train)\n",
    "val_data_generator = data_generator(val_captions, val_image_features, tokenizer, max_caption_length, batch_size_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf1218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, max_caption_length, cnn_output_dim):\n",
    "    input_image = Input(shape=(cnn_output_dim,), name='Features_Input')\n",
    "    fe1 = BatchNormalization()(input_image)\n",
    "    fe2 = Dense(256, activation='relu')(fe1) # Adding a Dense layer to the CNN output to match the decoder output size\n",
    "    fe3 = BatchNormalization()(fe2)\n",
    "\n",
    "    input_caption = Input(shape=(max_caption_length,), name='Sequence_Input')\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(input_caption)\n",
    "    se2 = LSTM(256)(se1)\n",
    "\n",
    "    decoder1 = add([fe3, se2])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax', name='Output_Layer')(decoder2)\n",
    "\n",
    "    model = Model(inputs=[input_image, input_caption], outputs=outputs, name='Image_Captioning')\n",
    "    return model\n",
    "\n",
    "caption_model = build_model(vocab_size, max_caption_length, cnn_output_dim)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01, clipnorm=1.0)\n",
    "caption_model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "caption_model.summary()\n",
    "\n",
    "plot_model(caption_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33/127 [======>.......................] - ETA: 12:42 - loss: 5.5394"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    return lr * tf.math.exp(-0.6)\n",
    "\n",
    "lr_schedule = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "history = caption_model.fit(train_data_generator, steps_per_epoch=len(train_captions) // batch_size_train,\n",
    "                        validation_data=val_data_generator, validation_steps=len(val_captions) // batch_size_val,\n",
    "                        epochs=15, callbacks=[early_stopping, lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee1883",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7), dpi=200)\n",
    "sns.set_style('whitegrid')\n",
    "plt.plot([x+1 for x in range(len(history.history['loss']))], history.history['loss'], color='#E74C3C', marker='o')\n",
    "plt.plot([x+1 for x in range(len(history.history['loss']))], history.history['val_loss'], color='#641E16', marker='h')\n",
    "plt.title('Train VS Validation', fontsize=15, fontweight='bold')\n",
    "plt.xticks(fontweight='bold')\n",
    "plt.yticks(fontweight='bold')\n",
    "plt.xlabel('Epoch', fontweight='bold')\n",
    "plt.ylabel('Loss', fontweight='bold')\n",
    "plt.legend(['Train Loss', 'Validation Loss'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8705e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_generator(image_features):\n",
    "    in_text = 'start '\n",
    "    for _ in range(max_caption_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_caption_length).reshape((1,max_caption_length))\n",
    "        prediction = caption_model.predict([image_features.reshape(1,cnn_output_dim), sequence], verbose=0)\n",
    "        idx = np.argmax(prediction)\n",
    "        word = tokenizer.index_word[idx]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'end':\n",
    "            break\n",
    "\n",
    "    in_text = in_text.replace('start ', '')\n",
    "    in_text = in_text.replace(' end', '')\n",
    "\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_generator(image_features, K_beams = 3, log = False):\n",
    "    start = [tokenizer.word_index['start']]\n",
    "    start_word = [[start, 0.0]]\n",
    "    for _ in range(max_caption_length):\n",
    "        temp = []\n",
    "        for s in start_word:\n",
    "            sequence  = pad_sequences([s[0]], maxlen=max_caption_length).reshape((1,max_caption_length))\n",
    "            preds = caption_model.predict([image_features.reshape(1,cnn_output_dim), sequence], verbose=0)\n",
    "            word_preds = np.argsort(preds[0])[-K_beams:]\n",
    "            for w in word_preds:\n",
    "                next_cap, prob = s[0][:], s[1]\n",
    "                next_cap.append(w)\n",
    "                if log:\n",
    "                    prob += np.log(preds[0][w]) # assign a probability to each K words\n",
    "                else:\n",
    "                    prob += preds[0][w]\n",
    "                temp.append([next_cap, prob])\n",
    "                \n",
    "        start_word = temp\n",
    "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
    "        start_word = start_word[-K_beams:]\n",
    "\n",
    "    start_word = start_word[-1][0]\n",
    "    captions_ = [tokenizer.index_word[i] for i in start_word]\n",
    "    final_caption = []\n",
    "    for i in captions_:\n",
    "        if i != 'end':\n",
    "            final_caption.append(i)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    final_caption = ' '.join(final_caption[1:])\n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc1f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLEU_score(actual, greedy, beam_search):\n",
    "    score_greedy_1 = corpus_bleu(actual, greedy, weights=(0.3, 0.3, 0.3, 0))\n",
    "    score_greedy_2 = corpus_bleu(actual, greedy, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    score_BS_1 = corpus_bleu(actual, beam_search, weights=(0.3, 0.3, 0.3, 0))\n",
    "    score_BS_2 = corpus_bleu(actual, beam_search, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "    return [\n",
    "        (f'BLEU-2 Greedy: {round(score_BS_2, 5)}'),\n",
    "        (f'BLEU-1 Greedy: {round(score_BS_1, 5)}'),\n",
    "        (f'Greedy: {greedy[0]}'),\n",
    "        (f'BLEU-2 Beam Search: {round(score_greedy_2, 5)}'),\n",
    "        (f'BLEU-1 Beam Search: {round(score_greedy_1, 5)}'),\n",
    "        (f'Beam Search:  {beam_search[0]}')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_captions = {}\n",
    "\n",
    "pbar = tqdm_notebook(total=len(test_image_features), position=0, leave=True, colour='green')\n",
    "for image_id in test_image_features:\n",
    "    cap = greedy_generator(test_image_features[image_id])\n",
    "    generated_captions[image_id] = cap\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(data, greedy_caps, beamS_generator, evaluator, num_of_images):\n",
    "    keys = list(data.keys()) \n",
    "    images = [np.random.choice(keys) for i in range(num_of_images)] # Randomly selected images\n",
    "    count = 1\n",
    "    fig = plt.figure(figsize=(6,20))\n",
    "    for filename in images:\n",
    "        actual_cap = data[filename]\n",
    "        actual_cap = [x.replace(\"start \", \"\") for x in actual_cap]\n",
    "        actual_cap = [x.replace(\" end\", \"\") for x in actual_cap]\n",
    "\n",
    "        greedy_cap = greedy_caps[filename]\n",
    "        beamS_cap = beamS_generator(test_image_features[filename])\n",
    "\n",
    "        caps_with_score = evaluator(actual_cap, [greedy_cap]*(len(actual_cap)), [beamS_cap]*(len(actual_cap)))\n",
    "\n",
    "        image_load = load_img(images_directory+filename, target_size=(199,199,3))\n",
    "        ax = fig.add_subplot(num_of_images,2,count,xticks=[],yticks=[])\n",
    "        ax.imshow(image_load)\n",
    "        count += 1\n",
    "\n",
    "        ax = fig.add_subplot(num_of_images,2,count)\n",
    "        plt.axis('off')\n",
    "        ax.plot()\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.set_ylim(0,len(caps_with_score))\n",
    "        for i, text in enumerate(caps_with_score):\n",
    "            ax.text(0,i,text,fontsize=10)\n",
    "        count += 1\n",
    "    plt.show()\n",
    "\n",
    "visualization(test_actual_captions, generated_captions, beam_search_generator, BLEU_score, 7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
